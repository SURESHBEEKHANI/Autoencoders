{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Autoencoders/blob/main/convolutional_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional autoencoder\n",
        "\n",
        "Apart from data compression, autoencoders can also be used for self-supervised image classification.\n",
        "\n",
        "The first layers of the encoder learn to recognize patterns in the data very well in order to compress the data. If you don't have access to much labelled data, but a lot of unlabelled data, it's possible to train an autoencoder and copy the first layers from the autoencoder to the classifier network. This can be compared to transfer learning, where you only have to train the head afterwards.\n",
        "\n",
        "It is well known that convolutional layers are perfect for finding patterns in data, so it's good to know they can also be used in autoencoders. To \"de-convolve\" images in the decoder, transposed convolutional layers are used. For more information on transposed convolutional layers, checkout [this blog post](https://rickwierenga.com/blog/s4tf/s4tf-gan.html) I have written which explains them in more detail. See the last section \"Autoencoders and GANs\" of this notebook for more information on the beautiful relation between GANs and autoencoders.\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "tLkyJrmf0b0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x"
      ],
      "metadata": {
        "id": "sm41_gLnUKSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline"
      ],
      "metadata": {
        "id": "sIgrCzEqUM2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WafsGOC7ivm"
      },
      "outputs": [],
      "source": [
        "#import the tensorflow import the keras\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "(x_train, _),(x_test, _)=keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "wSUm16mS8yYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normilize the data range into [0,1]\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0"
      ],
      "metadata": {
        "id": "m1Cd_Amh9CK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a convolutional encoder:"
      ],
      "metadata": {
        "id": "BPwrHYcr0zq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the encoder model using convolutional layers\n",
        "encoder = keras.models.Sequential([\n",
        "    # Reshape the input into a 28x28x1 image (grayscale)\n",
        "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
        "    # First convolutional layer with 16 filters, each of size 3x3, ReLU activation, and padding to maintain size\n",
        "    keras.layers.Conv2D(16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    # Max pooling layer with pool size 2x2 to downsample the spatial dimensions\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "    # Second convolutional layer with 32 filters, each of size 3x3, ReLU activation, and padding\n",
        "    keras.layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    # Max pooling layer\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "    # Third convolutional layer with 64 filters, each of size 3x3, ReLU activation, and padding\n",
        "    keras.layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    # Max pooling layer\n",
        "    keras.layers.MaxPool2D(pool_size=2)\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "9WL8WIfZ9Gn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights learnt by this encoder could be copied to the classifier."
      ],
      "metadata": {
        "id": "e5glX1FB1LiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the encoded representation of a single test image and print its shape\n",
        "encoder.predict(x_test[0].reshape((1, 28, 28))).shape\n"
      ],
      "metadata": {
        "id": "WZKOjY4mCO5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the decoder model using convolutional transpose layers\n",
        "decoder = keras.models.Sequential([\n",
        "    # Convolutional transpose layer with 32 filters, each of size 3x3, stride 2, ReLU activation, and valid padding\n",
        "    keras.layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=2, padding=\"valid\",\n",
        "                                 activation=\"relu\",\n",
        "                                 input_shape=[3, 3, 64]),\n",
        "    # Convolutional transpose layer with 16 filters, each of size 3x3, stride 2, ReLU activation, and same padding\n",
        "    keras.layers.Conv2DTranspose(16, kernel_size=(3, 3), strides=2, padding=\"same\",\n",
        "                                 activation=\"relu\"),\n",
        "    # Convolutional transpose layer with 1 filter, size 3x3, stride 2, sigmoid activation, and same padding\n",
        "    keras.layers.Conv2DTranspose(1, kernel_size=(3, 3), strides=2, padding=\"same\",\n",
        "                                 activation=\"sigmoid\"),\n",
        "    # Reshape the output into the original image shape (28x28)\n",
        "    keras.layers.Reshape([28, 28])\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "8qVfDl3OJKjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine the both encoder two decoder\n",
        "convolutional_autoencoder = keras.models.Sequential([encoder, decoder])\n",
        "\n",
        "# Print he model summary\n",
        "convolutional_autoencoder.summary()"
      ],
      "metadata": {
        "id": "MYOQjucRNL8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complie the convolutional_autoencoder\n",
        "convolutional_autoencoder.compile(loss=\"binary_crossentropy\",\n",
        "                                  optimizer='adam'\n",
        ")"
      ],
      "metadata": {
        "id": "44juitmKN3rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "convolutional_autoencoder.fit(x_train, x_train, epochs=10,validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "9GY_6-kWOBih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size for the plot\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.title(\"\"\"Original vs. Reconstructed\"\"\")\n",
        "\n",
        "\n",
        "# Iterate over 8 examples from the test set\n",
        "for i in range(4):\n",
        "    # Plot the original image from the test set\n",
        "    plt.subplot(2, 8, i + 1)\n",
        "    plt.title(\"Original\")\n",
        "    plt.imshow(x_test[i], cmap=\"binary\")\n",
        "\n",
        "\n",
        "    # Make a prediction using the convolutional autoencoder on the current test image\n",
        "    pred = convolutional_autoencoder.predict(x_test[i].reshape((1, 28, 28, 1)))\n",
        "\n",
        "    # Plot the reconstructed image by the convolutional autoencoder\n",
        "    plt.subplot(2, 8, i + 8 + 1)\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.imshow(pred.reshape((28, 28)), cmap=\"binary\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p6f_6FdgTvuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size for the plot\n",
        "figsize(15, 15)\n",
        "\n",
        "# Iterate over all filters in the last convolutional layer of the encoder\n",
        "for i in range(16):\n",
        "    # Plot each filter as a subplot in an 8x8 grid\n",
        "    subplot(8, 8, i+1)\n",
        "    # Display the weights (filters) of the convolutional layer\n",
        "    imshow(encoder.layers[-2].weights[0][:, :, 0, i])\n"
      ],
      "metadata": {
        "id": "p21FsJpwWbrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually not very pleasing, but proven to be effective as shown in the previous figure.\n",
        "\n",
        "$3 \\times 3 \\times 64=576$ is still less than $28 \\times 28 = 784$, thus creating a bottleneck, but much less compressed than the dense encoder making convolutional encoders less suitable for comporession. But thanks to their convolutional layers, they are great to use in cases where you want your autoencoder to find visual patterns in your data."
      ],
      "metadata": {
        "id": "kXM920jz1ddc"
      }
    }
  ]
}